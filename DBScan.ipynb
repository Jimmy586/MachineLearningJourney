{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jimmy586/MachineLearningJourney/blob/master/DBScan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB5QRhz6Ooc0",
        "outputId": "154d8f25-d6fc-49d7-d98d-d2ef560c1ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install opendatasets\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "rT6MZ-adOw1r",
        "outputId": "ec8111c1-96b6-4c4d-f35b-3fcab9c6a360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:Your Kaggle Key:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-83cf1ef5aed7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Data Preprocesiong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.kaggle.com/datasets/whenamancodes/predict-diabities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/predict-diabities/diabetes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opendatasets/__init__.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Check for a Kaggle dataset URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_kaggle_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdownload_kaggle_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Check for Google Drive URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36mdownload_kaggle_dataset\u001b[0;34m(dataset_url, data_dir, force, dry_run)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Kaggle username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_kaggle_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36m_get_kaggle_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_kaggle_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Kaggle Key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ],
      "source": [
        "### Data Preprocesiong\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/whenamancodes/predict-diabities\")\n",
        "\n",
        "df=pd.read_csv('/content/predict-diabities/diabetes.csv')\n",
        "\n",
        "df\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "y = df['Outcome']\n",
        "feature = df.iloc[:,0:8]\n",
        "feature = np.array(feature)\n",
        "\n",
        "norm = MinMaxScaler()\n",
        "norm_feature = norm.fit_transform(feature)\n",
        "y = np.array(y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TtW_G5hOiXv"
      },
      "outputs": [],
      "source": [
        "\n",
        "## DB_Scan Implementation\n",
        "\n",
        "### using Numpy + eps on euclidean distance\n",
        "\n",
        "def check_core_point(eps,minPts, data, index):\n",
        "    core=False\n",
        "    border=False\n",
        "    Noise=False\n",
        "    data_point = data[index]\n",
        "    indexes = np.arange( len(data) )\n",
        "    dist = np.linalg.norm(data - data_point,axis=1)\n",
        "    neighbours = (dist <= eps)\n",
        "    neighbours[index] = False\n",
        "    neighbours_indexes = indexes[neighbours]\n",
        "\n",
        "\n",
        "    if len(neighbours_indexes) >= minPts:\n",
        "        core=True\n",
        "    elif (len(neighbours_indexes) < minPts) and len(neighbours_indexes) > 0:\n",
        "        border=True\n",
        "    elif len(neighbours_indexes) == 0:\n",
        "        Noise=True\n",
        "    return (neighbours_indexes , core, border, Noise)\n",
        "\n",
        "### using Numpy + eps on every feature\n",
        "\n",
        "def check_core_point(eps,minPts, data, index):\n",
        "    core=False\n",
        "    border=False\n",
        "    Noise=False\n",
        "    data_point = data[index]\n",
        "    indexes = np.arange( len(data) )\n",
        "    dist = abs(data - data_point)\n",
        "    neighbours = np.all((dist <= eps), axis = 1)\n",
        "    neighbours[index] = False\n",
        "    neighbours_indexes = indexes[neighbours]\n",
        "\n",
        "\n",
        "    if len(neighbours_indexes) >= minPts:\n",
        "        core=True\n",
        "    elif (len(neighbours_indexes) < minPts) and len(neighbours_indexes) > 0:\n",
        "        border=True\n",
        "    elif len(neighbours_indexes) == 0:\n",
        "        Noise=True\n",
        "    return (neighbours_indexes , core, border, Noise)\n",
        "\n",
        "### using dataframe + eps on every feature\n",
        "\n",
        "def check_core_point(eps,minPts, df, index):\n",
        "    core=False\n",
        "    border=False\n",
        "    Noise=False\n",
        "    x = df.iloc[index][df.columns[0]]\n",
        "\n",
        "    temp = (np.abs(x - df[ df.columns[0] ]) <= eps) & (df.index != index)\n",
        "\n",
        "    for c in df.columns[1:]:\n",
        "      x = df.iloc[index][c]\n",
        "      temp =  ((np.abs(x - df[c]) <= eps) & temp)\n",
        "\n",
        "    temp = df[temp]\n",
        "\n",
        "    if len(temp) >= minPts:\n",
        "        core=True\n",
        "    elif (len(temp) < minPts) and len(temp) > 0:\n",
        "        border=True\n",
        "    elif len(temp) == 0:\n",
        "        Noise=True\n",
        "    return (temp , core, border, Noise)\n",
        "\n",
        "### main rules Numpy\n",
        "\n",
        "def cluster_with_stack(eps, minPts, df):\n",
        "\n",
        "    C = 1\n",
        "    current_stack = set()\n",
        "    unvisited = list(np.arange(len(df)))\n",
        "    clusters = []\n",
        "\n",
        "\n",
        "    while (len(unvisited) != 0):\n",
        "\n",
        "        first_point = True\n",
        "\n",
        "        current_stack.add(random.choice(unvisited))\n",
        "\n",
        "        while len(current_stack) != 0:\n",
        "\n",
        "            curr_idx = current_stack.pop()\n",
        "\n",
        "            neigh_indexes, iscore, isborder, isnoise = check_core_point(eps, minPts, df, curr_idx)\n",
        "\n",
        "            if (isborder & first_point):\n",
        "                clusters.append((curr_idx, 0))\n",
        "                clusters.extend(list(zip(neigh_indexes,[0 for _ in range(len(neigh_indexes))])))\n",
        "\n",
        "                unvisited.remove(curr_idx)\n",
        "                unvisited = [e for e in unvisited if e not in neigh_indexes]\n",
        "\n",
        "                continue\n",
        "\n",
        "            unvisited.remove(curr_idx)\n",
        "\n",
        "\n",
        "            neigh_indexes = set(neigh_indexes) & set(unvisited)\n",
        "\n",
        "            if iscore:\n",
        "                first_point = False\n",
        "\n",
        "                clusters.append((curr_idx,C))\n",
        "                current_stack.update(neigh_indexes)\n",
        "\n",
        "            elif isborder:\n",
        "                clusters.append((curr_idx,C))\n",
        "\n",
        "                continue\n",
        "\n",
        "            elif isnoise:\n",
        "                clusters.append((curr_idx, 0))\n",
        "\n",
        "                continue\n",
        "\n",
        "        if not first_point:\n",
        "            C+=1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "### main rules dataframe\n",
        "\n",
        "def cluster_with_stack(eps, minPts, df):\n",
        "\n",
        "    C = 1\n",
        "    current_stack = set()\n",
        "    unvisited = list(df.index)\n",
        "    clusters = []\n",
        "\n",
        "\n",
        "    while (len(unvisited) != 0):\n",
        "\n",
        "\n",
        "        first_point = True\n",
        "\n",
        "        current_stack.add(random.choice(unvisited))\n",
        "\n",
        "        while len(current_stack) != 0:\n",
        "\n",
        "\n",
        "            curr_idx = current_stack.pop()\n",
        "\n",
        "\n",
        "            neigh_indexes, iscore, isborder, isnoise = check_core_point(eps, minPts, df, curr_idx)\n",
        "\n",
        "            if (isborder & first_point):\n",
        "                clusters.append((curr_idx, 0))\n",
        "                clusters.extend(list(zip(neigh_indexes,[0 for _ in range(len(neigh_indexes))])))\n",
        "\n",
        "                unvisited.remove(curr_idx)\n",
        "                unvisited = [e for e in unvisited if e not in neigh_indexes]\n",
        "\n",
        "                continue\n",
        "\n",
        "            unvisited.remove(curr_idx)\n",
        "\n",
        "\n",
        "            neigh_indexes = set(neigh_indexes) & set(unvisited)\n",
        "\n",
        "            if iscore:\n",
        "                first_point = False\n",
        "\n",
        "                clusters.append((curr_idx,C))\n",
        "                current_stack.update(neigh_indexes)\n",
        "\n",
        "            elif isborder:\n",
        "                clusters.append((curr_idx,C))\n",
        "\n",
        "                continue\n",
        "\n",
        "            elif isnoise:\n",
        "                clusters.append((curr_idx, 0))\n",
        "\n",
        "                continue\n",
        "\n",
        "        if not first_point:\n",
        "            C+=1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "### Accuracy Function\n",
        "\n",
        "def acc(x,y):\n",
        "  noise_idx = np.array([t[0] for t in x.values if t[1] == 0])\n",
        "  c1_pred = np.array([t[0] for t in x.values if t[1] == 1])\n",
        "  c1_pred = np.concatenate((c1_pred,noise_idx),axis=0)\n",
        "  c2_pred = np.array([t[0] for t in x.values if t[1] == 2])\n",
        "\n",
        "  c1 = np.array([t for t,i in enumerate(y) if i == 0])\n",
        "  c2 = np.array([t for t,i in enumerate(y) if i == 1])\n",
        "\n",
        "  res = (len( np.intersect1d(c1 , c1_pred) ) + len(np.intersect1d(c2 , c2_pred)) ) /len(y) *100\n",
        "  if(res < 50): return 100 - res\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89NJCdLGPHoh"
      },
      "outputs": [],
      "source": [
        "### without PCA\n",
        "\n",
        "eps = 0.3\n",
        "minPts = 8\n",
        "\n",
        "clustered = cluster_with_stack(eps, minPts, norm_feature)\n",
        "\n",
        "idx , cluster = list(zip(*clustered))\n",
        "\n",
        "np.unique(cluster)\n",
        "\n",
        "cluster_df = pd.DataFrame(clustered, columns = [\"idx\", \"cluster\"])\n",
        "acc(cluster_df,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Au2gPUAO_pe"
      },
      "outputs": [],
      "source": [
        "\n",
        "## testing the code\n",
        "\n",
        "\n",
        "\n",
        "### with PCA\n",
        "\n",
        "cov_matrix = np.cov(feature.T)\n",
        "eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\n",
        "sorted_index = np.argsort(eigen_values)[::-1]\n",
        "print (sorted_index)\n",
        "\n",
        "sorted_eigenvalue = eigen_values[sorted_index]\n",
        "sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
        "\n",
        "n_components = 2\n",
        "eigenvector_subset = sorted_eigenvectors[:,0:n_components]\n",
        "\n",
        "X_reduced = np.dot(eigenvector_subset.transpose(),feature.transpose()).transpose()\n",
        "\n",
        "norm_pca = norm.fit_transform(X_reduced)\n",
        "\n",
        "eps = 0.2\n",
        "minPts = 3\n",
        "\n",
        "clustered = cluster_with_stack(eps, minPts, norm_pca)\n",
        "idx , cluster = list(zip(*clustered))\n",
        "np.unique(cluster)\n",
        "\n",
        "cluster_df = pd.DataFrame(clustered, columns = [\"idx\", \"cluster\"])\n",
        "\n",
        "acc(cluster_df,y)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}